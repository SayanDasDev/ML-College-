{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e-T4elIx9mNv",
      "metadata": {
        "id": "e-T4elIx9mNv"
      },
      "source": [
        "\n",
        "# Project 1: Comprehensive Regression Analysis\n",
        "### Course: Introduction to Machine Learning\n",
        "\n",
        "This notebook is designed to guide you through a comprehensive regression analysis using various techniques. You will explore different methods, implement regularization techniques, and evaluate the performance of your models using various metrics and computational time.\n",
        "\n",
        "\n",
        "## Submission Instructions\n",
        "\n",
        "Once you are finished, follow these steps:\n",
        "\n",
        "Make sure you have provided the team name, name of team members with IDs.\n",
        "\n",
        "Restart the kernel and re-run this notebook from beginning to end by going to Kernel > Restart Kernel and Run All Cells. If this process stops halfway through, that means there was an error. Correct the error and repeat Step 1 until the notebook runs from beginning to end. Double check that there is a number next to each code cell and that these numbers are in order. Then, submit your project as follows:\n",
        "\n",
        "Go to File > Print > Save as PDF. Double check that the entire notebook, from beginning to end, is in this PDF file. Upload the PDF and the notebook to Google Classroom.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dmmeNrUg9mNx",
      "metadata": {
        "id": "dmmeNrUg9mNx"
      },
      "source": [
        "### Team Name: **Insight Engineers**\n",
        "### Name and ID of Member 1: **Sayan Das** - ``B2430035``\n",
        "### Name and ID of Member 2: **Raihan Uddin** - ``B2430070``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "p1ZMSxPD9mNx",
      "metadata": {
        "id": "p1ZMSxPD9mNx"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "# Add all other libraries you would require\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error,mean_absolute_error, r2_score\n",
        "from copy import deepcopy\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NPyZ01tQ9mNy",
      "metadata": {
        "id": "NPyZ01tQ9mNy"
      },
      "source": [
        "\n",
        "## 1. Load the Dataset\n",
        "**Instruction:** Load the chosen dataset and display its basic information and statistics. You may use any well-known dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "1ccMmphv9mNy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "1ccMmphv9mNy",
        "outputId": "15ce4830-a5c8-4803-94d2-ee5c43f93454"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MedInc</th>\n",
              "      <th>HouseAge</th>\n",
              "      <th>AveRooms</th>\n",
              "      <th>AveBedrms</th>\n",
              "      <th>Population</th>\n",
              "      <th>AveOccup</th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8.3252</td>\n",
              "      <td>41.0</td>\n",
              "      <td>6.984127</td>\n",
              "      <td>1.023810</td>\n",
              "      <td>322.0</td>\n",
              "      <td>2.555556</td>\n",
              "      <td>37.88</td>\n",
              "      <td>-122.23</td>\n",
              "      <td>4.526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8.3014</td>\n",
              "      <td>21.0</td>\n",
              "      <td>6.238137</td>\n",
              "      <td>0.971880</td>\n",
              "      <td>2401.0</td>\n",
              "      <td>2.109842</td>\n",
              "      <td>37.86</td>\n",
              "      <td>-122.22</td>\n",
              "      <td>3.585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.2574</td>\n",
              "      <td>52.0</td>\n",
              "      <td>8.288136</td>\n",
              "      <td>1.073446</td>\n",
              "      <td>496.0</td>\n",
              "      <td>2.802260</td>\n",
              "      <td>37.85</td>\n",
              "      <td>-122.24</td>\n",
              "      <td>3.521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.6431</td>\n",
              "      <td>52.0</td>\n",
              "      <td>5.817352</td>\n",
              "      <td>1.073059</td>\n",
              "      <td>558.0</td>\n",
              "      <td>2.547945</td>\n",
              "      <td>37.85</td>\n",
              "      <td>-122.25</td>\n",
              "      <td>3.413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.8462</td>\n",
              "      <td>52.0</td>\n",
              "      <td>6.281853</td>\n",
              "      <td>1.081081</td>\n",
              "      <td>565.0</td>\n",
              "      <td>2.181467</td>\n",
              "      <td>37.85</td>\n",
              "      <td>-122.25</td>\n",
              "      <td>3.422</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
              "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
              "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
              "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
              "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
              "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
              "\n",
              "   Longitude  target  \n",
              "0    -122.23   4.526  \n",
              "1    -122.22   3.585  \n",
              "2    -122.24   3.521  \n",
              "3    -122.25   3.413  \n",
              "4    -122.25   3.422  "
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Loading a particular dataset\n",
        "\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "housing = fetch_california_housing()\n",
        "dataset=pd.DataFrame(housing.data,columns=housing.feature_names)\n",
        "dataset['target']=housing.target\n",
        "dataset.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "bOPa_7f99mNz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "bOPa_7f99mNz",
        "outputId": "072fad48-7290-4550-ea3a-e2f790ee0764"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>MedInc</th>\n",
              "      <td>20640.0</td>\n",
              "      <td>3.870671</td>\n",
              "      <td>1.899822</td>\n",
              "      <td>0.499900</td>\n",
              "      <td>2.563400</td>\n",
              "      <td>3.534800</td>\n",
              "      <td>4.743250</td>\n",
              "      <td>15.000100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HouseAge</th>\n",
              "      <td>20640.0</td>\n",
              "      <td>28.639486</td>\n",
              "      <td>12.585558</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>52.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AveRooms</th>\n",
              "      <td>20640.0</td>\n",
              "      <td>5.429000</td>\n",
              "      <td>2.474173</td>\n",
              "      <td>0.846154</td>\n",
              "      <td>4.440716</td>\n",
              "      <td>5.229129</td>\n",
              "      <td>6.052381</td>\n",
              "      <td>141.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AveBedrms</th>\n",
              "      <td>20640.0</td>\n",
              "      <td>1.096675</td>\n",
              "      <td>0.473911</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1.006079</td>\n",
              "      <td>1.048780</td>\n",
              "      <td>1.099526</td>\n",
              "      <td>34.066667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Population</th>\n",
              "      <td>20640.0</td>\n",
              "      <td>1425.476744</td>\n",
              "      <td>1132.462122</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>787.000000</td>\n",
              "      <td>1166.000000</td>\n",
              "      <td>1725.000000</td>\n",
              "      <td>35682.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AveOccup</th>\n",
              "      <td>20640.0</td>\n",
              "      <td>3.070655</td>\n",
              "      <td>10.386050</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>2.429741</td>\n",
              "      <td>2.818116</td>\n",
              "      <td>3.282261</td>\n",
              "      <td>1243.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Latitude</th>\n",
              "      <td>20640.0</td>\n",
              "      <td>35.631861</td>\n",
              "      <td>2.135952</td>\n",
              "      <td>32.540000</td>\n",
              "      <td>33.930000</td>\n",
              "      <td>34.260000</td>\n",
              "      <td>37.710000</td>\n",
              "      <td>41.950000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Longitude</th>\n",
              "      <td>20640.0</td>\n",
              "      <td>-119.569704</td>\n",
              "      <td>2.003532</td>\n",
              "      <td>-124.350000</td>\n",
              "      <td>-121.800000</td>\n",
              "      <td>-118.490000</td>\n",
              "      <td>-118.010000</td>\n",
              "      <td>-114.310000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <td>20640.0</td>\n",
              "      <td>2.068558</td>\n",
              "      <td>1.153956</td>\n",
              "      <td>0.149990</td>\n",
              "      <td>1.196000</td>\n",
              "      <td>1.797000</td>\n",
              "      <td>2.647250</td>\n",
              "      <td>5.000010</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              count         mean          std         min         25%  \\\n",
              "MedInc      20640.0     3.870671     1.899822    0.499900    2.563400   \n",
              "HouseAge    20640.0    28.639486    12.585558    1.000000   18.000000   \n",
              "AveRooms    20640.0     5.429000     2.474173    0.846154    4.440716   \n",
              "AveBedrms   20640.0     1.096675     0.473911    0.333333    1.006079   \n",
              "Population  20640.0  1425.476744  1132.462122    3.000000  787.000000   \n",
              "AveOccup    20640.0     3.070655    10.386050    0.692308    2.429741   \n",
              "Latitude    20640.0    35.631861     2.135952   32.540000   33.930000   \n",
              "Longitude   20640.0  -119.569704     2.003532 -124.350000 -121.800000   \n",
              "target      20640.0     2.068558     1.153956    0.149990    1.196000   \n",
              "\n",
              "                    50%          75%           max  \n",
              "MedInc         3.534800     4.743250     15.000100  \n",
              "HouseAge      29.000000    37.000000     52.000000  \n",
              "AveRooms       5.229129     6.052381    141.909091  \n",
              "AveBedrms      1.048780     1.099526     34.066667  \n",
              "Population  1166.000000  1725.000000  35682.000000  \n",
              "AveOccup       2.818116     3.282261   1243.333333  \n",
              "Latitude      34.260000    37.710000     41.950000  \n",
              "Longitude   -118.490000  -118.010000   -114.310000  \n",
              "target         1.797000     2.647250      5.000010  "
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Display basic statistics\n",
        "dataset.describe().T\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ME9folpL9mNz",
      "metadata": {
        "id": "ME9folpL9mNz"
      },
      "source": [
        "\n",
        "## 2. Data Preprocessing\n",
        "**Instruction:** Perform any necessary preprocessing steps, including handling missing values, encoding categorical variables, and scaling features if required.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "28964791",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "28964791",
        "outputId": "cef69b37-4400-46b9-9268-e54fda3a3957"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Missing Values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>MedInc</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HouseAge</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AveRooms</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AveBedrms</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Population</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AveOccup</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Latitude</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Longitude</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Missing Values\n",
              "MedInc                   0\n",
              "HouseAge                 0\n",
              "AveRooms                 0\n",
              "AveBedrms                0\n",
              "Population               0\n",
              "AveOccup                 0\n",
              "Latitude                 0\n",
              "Longitude                0\n",
              "target                   0"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Checking for missing values\n",
        "dataset.isnull().sum().to_frame('Missing Values')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "xebU2bnm9mN0",
      "metadata": {
        "id": "xebU2bnm9mN0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# removing data with missing values\n",
        "# there is no null values in the dataset, so nothing to remove\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "45067cda",
      "metadata": {
        "id": "45067cda"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Splitting the dataset into features (X) and target (y)\n",
        "# X = dataset['MedInc'].values.reshape(-1,1)\n",
        "# y = dataset['target'].values\n",
        "\n",
        "# X = dataset['median_income'].values.reshape(-1,1)\n",
        "# y = dataset['median_house_value'].values\n",
        "X = dataset.drop(columns=['target'])  # Features (all columns except 'target')\n",
        "y = dataset['target']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "0407ffc4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0407ffc4",
        "outputId": "86431bea-12a6-45e4-cc16-c220759c9839"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes of the resulting datasets:\n",
            "X_train: (16512, 8), y_train: (16512,)\n",
            "X_test: (4128, 8), y_test: (4128,)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Split the data into training and test sets (e.g., 80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(\"Shapes of the resulting datasets:\")\n",
        "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "87dc8917",
      "metadata": {
        "id": "87dc8917"
      },
      "outputs": [],
      "source": [
        "# Scaling features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test=scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "4e84fb06",
      "metadata": {
        "id": "4e84fb06"
      },
      "outputs": [],
      "source": [
        "matrices = pd.DataFrame(columns=['Train MSE', 'Test MSE', 'Train MAE', 'Test MAE', 'Train R2', 'Test R2', 'Time Taken'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xqHjmH7g9mN1",
      "metadata": {
        "id": "xqHjmH7g9mN1"
      },
      "source": [
        "\n",
        "## 3. Simple Linear Regression\n",
        "**Instruction:** Implement a simple linear regression model using scikit-learn.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "dxWvmca29mN1",
      "metadata": {
        "id": "dxWvmca29mN1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train MSE</th>\n",
              "      <th>Test MSE</th>\n",
              "      <th>Train MAE</th>\n",
              "      <th>Test MAE</th>\n",
              "      <th>Train R2</th>\n",
              "      <th>Test R2</th>\n",
              "      <th>Time Taken</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Simple Linear Regression</th>\n",
              "      <td>0.517933</td>\n",
              "      <td>0.555892</td>\n",
              "      <td>0.528628</td>\n",
              "      <td>0.5332</td>\n",
              "      <td>0.612551</td>\n",
              "      <td>0.575788</td>\n",
              "      <td>0.0051</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          Train MSE  Test MSE  Train MAE  Test MAE  Train R2  \\\n",
              "Simple Linear Regression   0.517933  0.555892   0.528628    0.5332  0.612551   \n",
              "\n",
              "                           Test R2  Time Taken  \n",
              "Simple Linear Regression  0.575788      0.0051  "
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Define the linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model on the training data\n",
        "start_time_slr = time.time()\n",
        "model.fit(X_train, y_train)\n",
        "end_time_slr = time.time()\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "# Predict on the training data for error calculation\n",
        "y_train_pred = model.predict(X_train)\n",
        "\n",
        "# Calculate performance metrics [MSE, MAE, R^2]\n",
        "mse_train = mean_squared_error(y_train, y_train_pred)\n",
        "mse_test = mean_squared_error(y_test, y_pred)\n",
        "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
        "mae_test = mean_absolute_error(y_test, y_pred)\n",
        "r2_train = r2_score(y_train, y_train_pred)\n",
        "r2_test = r2_score(y_test, y_pred)\n",
        "time_taken_slr = end_time_slr - start_time_slr\n",
        "\n",
        "# Print the metrics\n",
        "matrices.loc['Simple Linear Regression'] = [mse_train, mse_test, mae_train, mae_test, r2_train, r2_test, time_taken_slr]\n",
        "\n",
        "matrices"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NuIGoLkH9mN1",
      "metadata": {
        "id": "NuIGoLkH9mN1"
      },
      "source": [
        "\n",
        "## 4. Polynomial Regression\n",
        "**Instruction:** Implement polynomial regression for degrees 2, 3, and 4.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "e64375fe",
      "metadata": {
        "id": "e64375fe"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train MSE</th>\n",
              "      <th>Test MSE</th>\n",
              "      <th>Train MAE</th>\n",
              "      <th>Test MAE</th>\n",
              "      <th>Train R2</th>\n",
              "      <th>Test R2</th>\n",
              "      <th>Time Taken</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Simple Linear Regression</th>\n",
              "      <td>0.517933</td>\n",
              "      <td>0.555892</td>\n",
              "      <td>0.528628</td>\n",
              "      <td>0.533200</td>\n",
              "      <td>0.612551</td>\n",
              "      <td>0.575788</td>\n",
              "      <td>0.005100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Polynomial Linear Regression (degree - 2)</th>\n",
              "      <td>0.420727</td>\n",
              "      <td>0.464302</td>\n",
              "      <td>0.460838</td>\n",
              "      <td>0.467001</td>\n",
              "      <td>0.685268</td>\n",
              "      <td>0.645682</td>\n",
              "      <td>0.036937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Polynomial Linear Regression (degree - 3)</th>\n",
              "      <td>0.342608</td>\n",
              "      <td>20.009742</td>\n",
              "      <td>0.416137</td>\n",
              "      <td>0.525636</td>\n",
              "      <td>0.743706</td>\n",
              "      <td>-14.269845</td>\n",
              "      <td>0.199386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Polynomial Linear Regression (degree - 4)</th>\n",
              "      <td>0.470660</td>\n",
              "      <td>2396.745690</td>\n",
              "      <td>0.471706</td>\n",
              "      <td>1.605144</td>\n",
              "      <td>0.647915</td>\n",
              "      <td>-1828.005852</td>\n",
              "      <td>0.517941</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           Train MSE     Test MSE  Train MAE  \\\n",
              "Simple Linear Regression                    0.517933     0.555892   0.528628   \n",
              "Polynomial Linear Regression (degree - 2)   0.420727     0.464302   0.460838   \n",
              "Polynomial Linear Regression (degree - 3)   0.342608    20.009742   0.416137   \n",
              "Polynomial Linear Regression (degree - 4)   0.470660  2396.745690   0.471706   \n",
              "\n",
              "                                           Test MAE  Train R2      Test R2  \\\n",
              "Simple Linear Regression                   0.533200  0.612551     0.575788   \n",
              "Polynomial Linear Regression (degree - 2)  0.467001  0.685268     0.645682   \n",
              "Polynomial Linear Regression (degree - 3)  0.525636  0.743706   -14.269845   \n",
              "Polynomial Linear Regression (degree - 4)  1.605144  0.647915 -1828.005852   \n",
              "\n",
              "                                           Time Taken  \n",
              "Simple Linear Regression                     0.005100  \n",
              "Polynomial Linear Regression (degree - 2)    0.036937  \n",
              "Polynomial Linear Regression (degree - 3)    0.199386  \n",
              "Polynomial Linear Regression (degree - 4)    0.517941  "
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Assume X and y are already defined (features and target)\n",
        "\n",
        "def polynomial_regression(degree):\n",
        "    # Split the data into training and test sets first\n",
        "    X_train_poly, X_test_poly, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define polynomial features and apply only to the training set\n",
        "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    X_train_poly = poly.fit_transform(X_train_poly)\n",
        "    X_test_poly = poly.transform(X_test_poly)  # Transform the test set using the same polynomial features\n",
        "\n",
        "    # Fit the linear model on polynomial features\n",
        "    poly_model = LinearRegression()\n",
        "    start_time_poly = time.time()\n",
        "    poly_model.fit(X_train_poly, y_train)\n",
        "    end_time_poly = time.time()\n",
        "\n",
        "    # Predict and evaluate performance\n",
        "    y_train_pred_poly = poly_model.predict(X_train_poly)\n",
        "    y_test_pred_poly = poly_model.predict(X_test_poly)\n",
        "\n",
        "    # Calculate performance metrics for polynomial regression\n",
        "    mse_train_poly = mean_squared_error(y_train, y_train_pred_poly)\n",
        "    mse_test_poly = mean_squared_error(y_test, y_test_pred_poly)\n",
        "    mae_train_poly = mean_absolute_error(y_train, y_train_pred_poly)\n",
        "    mae_test_poly = mean_absolute_error(y_test, y_test_pred_poly)\n",
        "    r2_train_poly = r2_score(y_train, y_train_pred_poly)\n",
        "    r2_test_poly = r2_score(y_test, y_test_pred_poly)\n",
        "    time_taken_poly = end_time_poly - start_time_poly\n",
        "\n",
        "    # Store the results in the matrices DataFrame\n",
        "    model_name = f'Polynomial Linear Regression (degree - {degree})'\n",
        "    matrices.loc[model_name] = [mse_train_poly, mse_test_poly, mae_train_poly, mae_test_poly, r2_train_poly, r2_test_poly, time_taken_poly]\n",
        "\n",
        "# Test polynomial regression for different degrees\n",
        "for degree in [2, 3, 4]:\n",
        "    polynomial_regression(degree)\n",
        "    \n",
        "matrices"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GD1e7B649mN1",
      "metadata": {
        "id": "GD1e7B649mN1"
      },
      "source": [
        "\n",
        "## 5. Gradient Descent Methods\n",
        "**Instruction:** Implement batch, stochastic, and mini-batch gradient descent for linear regression.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "fXrBqRt4hveW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXrBqRt4hveW",
        "outputId": "ab2fe9de-a622-4f02-c965-48499caaca37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch Gradient Descent:\n",
            "\n",
            "Epoch 0, \tTrain Cost: 2.8148711615515656, \tTest Cost: 2.7618119781737773\n",
            "Epoch 100, \tTrain Cost: 2.3607292301908327, \tTest Cost: 2.3176633999340517\n",
            "Epoch 200, \tTrain Cost: 1.9896530312647223, \tTest Cost: 1.9548092348335955\n",
            "Epoch 300, \tTrain Cost: 1.6862997481225512, \tTest Cost: 1.6582203113363985\n",
            "Epoch 400, \tTrain Cost: 1.4381947698129598, \tTest Cost: 1.4156843436790194\n",
            "Epoch 500, \tTrain Cost: 1.2351854513104494, \tTest Cost: 1.217265836840618\n",
            "Epoch 600, \tTrain Cost: 1.0690019221898703, \tTest Cost: 1.054873374000918\n",
            "Epoch 700, \tTrain Cost: 0.9329030654137263, \tTest Cost: 0.921911761280444\n",
            "Epoch 800, \tTrain Cost: 0.8213905212227512, \tTest Cost: 0.81300157564019\n",
            "Epoch 900, \tTrain Cost: 0.7299772071054669, \tTest Cost: 0.7237524952139973\n",
            "Epoch 1000, \tTrain Cost: 0.6549996572834759, \tTest Cost: 0.6505797138266712\n",
            "Epoch 1100, \tTrain Cost: 0.5934656747700666, \tTest Cost: 0.5905549876925769\n",
            "Epoch 1200, \tTrain Cost: 0.5429305040321135, \tTest Cost: 0.5412856020916226\n",
            "Epoch 1300, \tTrain Cost: 0.5013960828856308, \tTest Cost: 0.5008159030200637\n",
            "Epoch 1400, \tTrain Cost: 0.4672290011863204, \tTest Cost: 0.46754710448977177\n",
            "Epoch 1500, \tTrain Cost: 0.4390936436441713, \tTest Cost: 0.4401719238678303\n",
            "Epoch 1600, \tTrain Cost: 0.41589767229774105, \tTest Cost: 0.4176212659762865\n",
            "Epoch 1700, \tTrain Cost: 0.39674754734766055, \tTest Cost: 0.39902070978859916\n",
            "Epoch 1800, \tTrain Cost: 0.380912221382512, \tTest Cost: 0.3836549785394977\n",
            "Epoch 1900, \tTrain Cost: 0.36779349347207596, \tTest Cost: 0.37093891723351274\n",
            "Epoch 2000, \tTrain Cost: 0.3569017933172485, \tTest Cost: 0.3603937781713366\n",
            "Epoch 2100, \tTrain Cost: 0.34783639513640524, \tTest Cost: 0.3516278386802737\n",
            "Epoch 2200, \tTrain Cost: 0.34026924691235516, \tTest Cost: 0.344320556304165\n",
            "Epoch 2300, \tTrain Cost: 0.3339317515036505, \tTest Cost: 0.3382096136231185\n",
            "Epoch 2400, \tTrain Cost: 0.32860395870295356, \tTest Cost: 0.3330803242568834\n",
            "Epoch 2500, \tTrain Cost: 0.3241057270169343, \tTest Cost: 0.32875696873921034\n",
            "Epoch 2600, \tTrain Cost: 0.3202894950930348, \tTest Cost: 0.3250957080633142\n",
            "Epoch 2700, \tTrain Cost: 0.3170343688272283, \tTest Cost: 0.3219787871894044\n",
            "Epoch 2800, \tTrain Cost: 0.31424128407679597, \tTest Cost: 0.3193097934137932\n",
            "Epoch 2900, \tTrain Cost: 0.31182904885590834, \tTest Cost: 0.31700977743949915\n",
            "\n",
            "Batch Gradient Descent Final Train Cost: 0.3097507164443857\n",
            "Batch Gradient Descent Final Test Cost: 0.3150327131346944\n",
            "Batch Gradient Descent Best Epoch: 2999\n",
            "Batch Gradient Descent Time Taken: 0.29 seconds\n",
            "\n",
            "Stochastic Gradient Descent:\n",
            "\n",
            "Epoch 0, \tTrain Cost: 0.40314797292497945, \tTest Cost: 0.4053899455818228\n",
            "Epoch 100, \tTrain Cost: 0.2591163427735514, \tTest Cost: 0.27614874686248275\n",
            "Epoch 200, \tTrain Cost: 0.25919930688961923, \tTest Cost: 0.2809542020152388\n",
            "Epoch 300, \tTrain Cost: 0.2590441699477945, \tTest Cost: 0.27797716066219613\n",
            "Epoch 400, \tTrain Cost: 0.25907118356866754, \tTest Cost: 0.2798538769621824\n",
            "Epoch 500, \tTrain Cost: 0.2590533468180606, \tTest Cost: 0.2797593950252377\n",
            "Epoch 600, \tTrain Cost: 0.25901255117663946, \tTest Cost: 0.27696219845918074\n",
            "Epoch 700, \tTrain Cost: 0.2590454631947876, \tTest Cost: 0.27807093041447445\n",
            "Epoch 800, \tTrain Cost: 0.2590328056667652, \tTest Cost: 0.2767341019259223\n",
            "Epoch 900, \tTrain Cost: 0.25905496272994566, \tTest Cost: 0.27984574938551915\n",
            "\n",
            "Stochastic Gradient Descent Final Train Cost: 0.259018351238257\n",
            "Stochastic Gradient Descent Final Test Cost: 0.2784068751273489\n",
            "Stochastic Gradient Descent Best Epoch: 126\n",
            "Stochastic Gradient Descent Time Taken: 79.96 seconds\n",
            "\n",
            "Mini-Batch Gradient Descent:\n",
            "\n",
            "Epoch 0, \tTrain Cost: 0.6326044982364547, \tTest Cost: 0.6291137829283383\n",
            "Epoch 100, \tTrain Cost: 0.2590239305318625, \tTest Cost: 0.2794861107659521\n",
            "Epoch 200, \tTrain Cost: 0.25898134592900096, \tTest Cost: 0.2781072956257453\n",
            "Epoch 300, \tTrain Cost: 0.2590025793634248, \tTest Cost: 0.27683961836049664\n",
            "Epoch 400, \tTrain Cost: 0.25911121705262863, \tTest Cost: 0.2757447474795142\n",
            "Epoch 500, \tTrain Cost: 0.2589911450155486, \tTest Cost: 0.2783349592587981\n",
            "Epoch 600, \tTrain Cost: 0.2589897639756886, \tTest Cost: 0.27718496618324306\n",
            "Epoch 700, \tTrain Cost: 0.25897988903116237, \tTest Cost: 0.27717637761325326\n",
            "Epoch 800, \tTrain Cost: 0.259024914826843, \tTest Cost: 0.279164677781386\n",
            "Epoch 900, \tTrain Cost: 0.25909256024515864, \tTest Cost: 0.28030451312829197\n",
            "\n",
            "Mini-Batch Gradient Descent Final Train Cost: 0.2589748450599067\n",
            "Mini-Batch Gradient Descent Final Test Cost: 0.2783953271494115\n",
            "Mini-Batch Gradient Descent Best Epoch: 361\n",
            "Mini-Batch Gradient Descent Time Taken: 11.69 seconds\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Batch Gradient Descent Function\n",
        "def batch_gradient_descent(X_train, y_train, X_test, y_test, alpha, epochs):\n",
        "    m, n = X_train.shape\n",
        "    theta = np.zeros((n, 1))\n",
        "    y_train = y_train.reshape(m, 1)\n",
        "    cost_history_train = []\n",
        "    cost_history_test = []\n",
        "    best_cost = float('inf')\n",
        "    best_epoch = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        h_train = X_train.dot(theta)\n",
        "        gradient = (1/m) * X_train.T.dot(h_train - y_train)\n",
        "        theta -= alpha * gradient\n",
        "        cost_train = (1/(2*m)) * np.sum((h_train - y_train)**2)\n",
        "        cost_history_train.append(cost_train)\n",
        "\n",
        "        h_test = X_test.dot(theta)\n",
        "        cost_test = (1/(2*X_test.shape[0])) * np.sum((h_test - y_test.reshape(-1, 1))**2)\n",
        "        cost_history_test.append(cost_test)\n",
        "\n",
        "        if cost_train < best_cost:\n",
        "            best_cost = cost_train\n",
        "            best_epoch = epoch\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}, \\tTrain Cost: {cost_train}, \\tTest Cost: {cost_test}\")\n",
        "\n",
        "    return theta, cost_history_train, cost_history_test, best_epoch\n",
        "\n",
        "# Stochastic Gradient Descent Function\n",
        "def stochastic_gradient_descent(X_train, y_train, X_test, y_test, alpha, epochs):\n",
        "    m, n = X_train.shape\n",
        "    theta = np.zeros((n, 1))\n",
        "    y_train = y_train.reshape(m, 1)\n",
        "    cost_history_train = []\n",
        "    cost_history_test = []\n",
        "    best_cost = float('inf')\n",
        "    best_epoch = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        indices = np.arange(m)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        for i in indices:\n",
        "            X_i = X_train[i:i+1]\n",
        "            y_i = y_train[i:i+1]\n",
        "            h = X_i.dot(theta)\n",
        "            gradient = X_i.T.dot(h - y_i)\n",
        "            theta -= alpha * gradient\n",
        "\n",
        "        h_train = X_train.dot(theta)\n",
        "        cost_train = (1/(2*m)) * np.sum((h_train - y_train)**2)\n",
        "        cost_history_train.append(cost_train)\n",
        "\n",
        "        h_test = X_test.dot(theta)\n",
        "        cost_test = (1/(2*X_test.shape[0])) * np.sum((h_test - y_test.reshape(-1, 1))**2)\n",
        "        cost_history_test.append(cost_test)\n",
        "\n",
        "        if cost_train < best_cost:\n",
        "            best_cost = cost_train\n",
        "            best_epoch = epoch\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}, \\tTrain Cost: {cost_train}, \\tTest Cost: {cost_test}\")\n",
        "\n",
        "    return theta, cost_history_train, cost_history_test, best_epoch\n",
        "\n",
        "# Mini-Batch Gradient Descent Function\n",
        "def mini_batch_gradient_descent(X_train, y_train, X_test, y_test, alpha, epochs, batch_size):\n",
        "    m, n = X_train.shape\n",
        "    theta = np.zeros((n, 1))\n",
        "    y_train = y_train.reshape(m, 1)\n",
        "    cost_history_train = []\n",
        "    cost_history_test = []\n",
        "    best_cost = float('inf')\n",
        "    best_epoch = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        indices = np.arange(m)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        for start in range(0, m, batch_size):\n",
        "            end = min(start + batch_size, m)\n",
        "            X_batch = X_train[indices[start:end]]\n",
        "            y_batch = y_train[indices[start:end]]\n",
        "            h_batch = X_batch.dot(theta)\n",
        "            gradient = (1/len(y_batch)) * X_batch.T.dot(h_batch - y_batch)\n",
        "            theta -= alpha * gradient\n",
        "\n",
        "        h_train = X_train.dot(theta)\n",
        "        cost_train = (1/(2*m)) * np.sum((h_train - y_train)**2)\n",
        "        cost_history_train.append(cost_train)\n",
        "\n",
        "        h_test = X_test.dot(theta)\n",
        "        cost_test = (1/(2*X_test.shape[0])) * np.sum((h_test - y_test.reshape(-1, 1))**2)\n",
        "        cost_history_test.append(cost_test)\n",
        "\n",
        "        if cost_train < best_cost:\n",
        "            best_cost = cost_train\n",
        "            best_epoch = epoch\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}, \\tTrain Cost: {cost_train}, \\tTest Cost: {cost_test}\")\n",
        "\n",
        "    return theta, cost_history_train, cost_history_test, best_epoch\n",
        "\n",
        "# Add bias term to features\n",
        "X_train = np.c_[np.ones(X_train.shape[0]), X_train]\n",
        "X_test = np.c_[np.ones(X_test.shape[0]), X_test]\n",
        "\n",
        "# Convert target to numpy array\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Set learning rate, number of epochs, and batch size for Mini-Batch\n",
        "alpha = 0.001\n",
        "epochs = 3000\n",
        "batch_size = 16\n",
        "\n",
        "# Run Batch Gradient Descent\n",
        "print(\"Batch Gradient Descent:\\n\")\n",
        "start_time_bgd = time.time()\n",
        "theta_batch, cost_history_bgd_train, cost_history_bgd_test, best_epoch_batch = batch_gradient_descent(X_train, y_train, X_test, y_test, alpha, epochs)\n",
        "end_time_bgd = time.time()\n",
        "print(f\"\\nBatch Gradient Descent Final Train Cost: {cost_history_bgd_train[-1]}\")\n",
        "print(f\"Batch Gradient Descent Final Test Cost: {cost_history_bgd_test[-1]}\")\n",
        "print(f\"Batch Gradient Descent Best Epoch: {best_epoch_batch}\")\n",
        "print(f\"Batch Gradient Descent Time Taken: {end_time_bgd - start_time_bgd:.2f} seconds\\n\")\n",
        "\n",
        "# Run Stochastic Gradient Descent\n",
        "alpha = 0.0001\n",
        "epochs = 1000\n",
        "print(\"Stochastic Gradient Descent:\\n\")\n",
        "start_time_sgd = time.time()\n",
        "theta_sgd, cost_history_sgd_train, cost_history_sgd_test, best_epoch_sgd = stochastic_gradient_descent(X_train, y_train, X_test, y_test, alpha, epochs)\n",
        "end_time_sgd = time.time()\n",
        "print(f\"\\nStochastic Gradient Descent Final Train Cost: {cost_history_sgd_train[-1]}\")\n",
        "print(f\"Stochastic Gradient Descent Final Test Cost: {cost_history_sgd_test[-1]}\")\n",
        "print(f\"Stochastic Gradient Descent Best Epoch: {best_epoch_sgd}\")\n",
        "print(f\"Stochastic Gradient Descent Time Taken: {end_time_sgd - start_time_sgd:.2f} seconds\\n\")\n",
        "\n",
        "# Run Mini-Batch Gradient Descent\n",
        "alpha = 0.001\n",
        "epochs = 1000\n",
        "batch_size = 16\n",
        "print(\"Mini-Batch Gradient Descent:\\n\")\n",
        "start_time_mb = time.time()\n",
        "theta_mini_batch, cost_history_mini_batch_train, cost_history_mini_batch_test, best_epoch_mini_batch = mini_batch_gradient_descent(X_train, y_train, X_test, y_test, alpha, epochs, batch_size)\n",
        "end_time_mb = time.time()\n",
        "print(f\"\\nMini-Batch Gradient Descent Final Train Cost: {cost_history_mini_batch_train[-1]}\")\n",
        "print(f\"Mini-Batch Gradient Descent Final Test Cost: {cost_history_mini_batch_test[-1]}\")\n",
        "print(f\"Mini-Batch Gradient Descent Best Epoch: {best_epoch_mini_batch}\")\n",
        "print(f\"Mini-Batch Gradient Descent Time Taken: {end_time_mb - start_time_mb:.2f} seconds\\n\")\n",
        "\n",
        "# time dictionary\n",
        "gd_time = {\n",
        "    'Batch Gradient Descent': end_time_bgd - start_time_bgd,\n",
        "    'Stochastic Gradient Descent': end_time_sgd - start_time_sgd,\n",
        "    'Mini-Batch Gradient Descent': end_time_mb - start_time_mb\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41h_LKBa9mN2",
      "metadata": {
        "id": "41h_LKBa9mN2"
      },
      "source": [
        "\n",
        "## 6. Regularization Techniques (Ridge,  Lasso, Elastic Net Regression and Early Stopping)\n",
        "**Instruction:** Implement Ridge, Lasso regression, Elastic Net Regression, and Early Stopping using scikit-learn and compare the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "x3xm2aAYdAOw",
      "metadata": {
        "id": "x3xm2aAYdAOw"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train MSE</th>\n",
              "      <th>Test MSE</th>\n",
              "      <th>Train MAE</th>\n",
              "      <th>Test MAE</th>\n",
              "      <th>Train R2</th>\n",
              "      <th>Test R2</th>\n",
              "      <th>Time Taken</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Simple Linear Regression</th>\n",
              "      <td>5.179331e-01</td>\n",
              "      <td>5.558916e-01</td>\n",
              "      <td>0.528628</td>\n",
              "      <td>0.533200</td>\n",
              "      <td>6.125512e-01</td>\n",
              "      <td>5.757877e-01</td>\n",
              "      <td>0.005100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Polynomial Linear Regression (degree - 2)</th>\n",
              "      <td>4.207266e-01</td>\n",
              "      <td>4.643015e-01</td>\n",
              "      <td>0.460838</td>\n",
              "      <td>0.467001</td>\n",
              "      <td>6.852682e-01</td>\n",
              "      <td>6.456820e-01</td>\n",
              "      <td>0.036937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Polynomial Linear Regression (degree - 3)</th>\n",
              "      <td>3.426077e-01</td>\n",
              "      <td>2.000974e+01</td>\n",
              "      <td>0.416137</td>\n",
              "      <td>0.525636</td>\n",
              "      <td>7.437064e-01</td>\n",
              "      <td>-1.426985e+01</td>\n",
              "      <td>0.199386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Polynomial Linear Regression (degree - 4)</th>\n",
              "      <td>4.706597e-01</td>\n",
              "      <td>2.396746e+03</td>\n",
              "      <td>0.471706</td>\n",
              "      <td>1.605144</td>\n",
              "      <td>6.479149e-01</td>\n",
              "      <td>-1.828006e+03</td>\n",
              "      <td>0.517941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ridge Regression</th>\n",
              "      <td>5.179332e-01</td>\n",
              "      <td>5.558549e-01</td>\n",
              "      <td>0.528624</td>\n",
              "      <td>0.533193</td>\n",
              "      <td>6.125511e-01</td>\n",
              "      <td>5.758157e-01</td>\n",
              "      <td>0.004001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lasso Regression</th>\n",
              "      <td>1.336778e+00</td>\n",
              "      <td>1.310696e+00</td>\n",
              "      <td>0.913911</td>\n",
              "      <td>0.906069</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-2.190871e-04</td>\n",
              "      <td>0.003029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Elastic Net Regression</th>\n",
              "      <td>1.058553e+00</td>\n",
              "      <td>1.044231e+00</td>\n",
              "      <td>0.812107</td>\n",
              "      <td>0.805995</td>\n",
              "      <td>2.081313e-01</td>\n",
              "      <td>2.031260e-01</td>\n",
              "      <td>0.002012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Early Stopping</th>\n",
              "      <td>2.811376e+09</td>\n",
              "      <td>6.395922e+09</td>\n",
              "      <td>2883.486634</td>\n",
              "      <td>3190.668525</td>\n",
              "      <td>-2.103098e+09</td>\n",
              "      <td>-4.880859e+09</td>\n",
              "      <td>3.310140</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Train MSE      Test MSE  \\\n",
              "Simple Linear Regression                   5.179331e-01  5.558916e-01   \n",
              "Polynomial Linear Regression (degree - 2)  4.207266e-01  4.643015e-01   \n",
              "Polynomial Linear Regression (degree - 3)  3.426077e-01  2.000974e+01   \n",
              "Polynomial Linear Regression (degree - 4)  4.706597e-01  2.396746e+03   \n",
              "Ridge Regression                           5.179332e-01  5.558549e-01   \n",
              "Lasso Regression                           1.336778e+00  1.310696e+00   \n",
              "Elastic Net Regression                     1.058553e+00  1.044231e+00   \n",
              "Early Stopping                             2.811376e+09  6.395922e+09   \n",
              "\n",
              "                                             Train MAE     Test MAE  \\\n",
              "Simple Linear Regression                      0.528628     0.533200   \n",
              "Polynomial Linear Regression (degree - 2)     0.460838     0.467001   \n",
              "Polynomial Linear Regression (degree - 3)     0.416137     0.525636   \n",
              "Polynomial Linear Regression (degree - 4)     0.471706     1.605144   \n",
              "Ridge Regression                              0.528624     0.533193   \n",
              "Lasso Regression                              0.913911     0.906069   \n",
              "Elastic Net Regression                        0.812107     0.805995   \n",
              "Early Stopping                             2883.486634  3190.668525   \n",
              "\n",
              "                                               Train R2       Test R2  \\\n",
              "Simple Linear Regression                   6.125512e-01  5.757877e-01   \n",
              "Polynomial Linear Regression (degree - 2)  6.852682e-01  6.456820e-01   \n",
              "Polynomial Linear Regression (degree - 3)  7.437064e-01 -1.426985e+01   \n",
              "Polynomial Linear Regression (degree - 4)  6.479149e-01 -1.828006e+03   \n",
              "Ridge Regression                           6.125511e-01  5.758157e-01   \n",
              "Lasso Regression                           0.000000e+00 -2.190871e-04   \n",
              "Elastic Net Regression                     2.081313e-01  2.031260e-01   \n",
              "Early Stopping                            -2.103098e+09 -4.880859e+09   \n",
              "\n",
              "                                           Time Taken  \n",
              "Simple Linear Regression                     0.005100  \n",
              "Polynomial Linear Regression (degree - 2)    0.036937  \n",
              "Polynomial Linear Regression (degree - 3)    0.199386  \n",
              "Polynomial Linear Regression (degree - 4)    0.517941  \n",
              "Ridge Regression                             0.004001  \n",
              "Lasso Regression                             0.003029  \n",
              "Elastic Net Regression                       0.002012  \n",
              "Early Stopping                               3.310140  "
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Ridge Regression\n",
        "ridge_model = Ridge(alpha=1.0)\n",
        "start_time_ridge = time.time()\n",
        "ridge_model.fit(X_train, y_train)\n",
        "end_time_ridge = time.time()\n",
        "y_pred_ridge_train = ridge_model.predict(X_train)\n",
        "y_pred_ridge_test = ridge_model.predict(X_test)\n",
        "\n",
        "# Lasso Regression\n",
        "lasso_model = Lasso(alpha=1.0)\n",
        "start_time_lasso = time.time()\n",
        "lasso_model.fit(X_train, y_train)\n",
        "end_time_lasso = time.time()\n",
        "y_pred_lasso_train = lasso_model.predict(X_train)\n",
        "y_pred_lasso_test = lasso_model.predict(X_test)\n",
        "\n",
        "# Elastic Net Regression\n",
        "elastic_net_model = ElasticNet(alpha=1.0)\n",
        "start_time_elastic_net = time.time()\n",
        "elastic_net_model.fit(X_train, y_train)\n",
        "end_time_elastic_net = time.time()\n",
        "y_pred_elastic_net_train = elastic_net_model.predict(X_train)\n",
        "y_pred_elastic_net_test = elastic_net_model.predict(X_test)\n",
        "\n",
        "# Early Stopping\n",
        "start_time_es = time.time()\n",
        "X_train_es, X_valid, y_train_es, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preprocessing pipeline: Polynomial features and scaling\n",
        "degree = 2\n",
        "preprocessing = make_pipeline(\n",
        "    PolynomialFeatures(degree=degree, include_bias=False),\n",
        "    StandardScaler())\n",
        "X_train_prep = preprocessing.fit_transform(X_train_es)\n",
        "X_valid_prep = preprocessing.transform(X_valid)\n",
        "\n",
        "# Initialize the SGDRegressor\n",
        "sgd_reg = SGDRegressor(penalty=None, eta0=0.01, random_state=42)\n",
        "\n",
        "# Parameters for training\n",
        "n_epochs = 500\n",
        "best_valid_rmse = float('inf')\n",
        "best_model = None\n",
        "\n",
        "# Training loop with early stopping\n",
        "for epoch in range(n_epochs):\n",
        "    sgd_reg.partial_fit(X_train_prep, y_train_es.ravel())\n",
        "    y_valid_predict = sgd_reg.predict(X_valid_prep)\n",
        "    valid_mse = mean_squared_error(y_valid, y_valid_predict)\n",
        "    val_error = np.sqrt(valid_mse)\n",
        "\n",
        "    if val_error < best_valid_rmse:\n",
        "        best_valid_rmse = val_error\n",
        "        best_model = deepcopy(sgd_reg)\n",
        "\n",
        "end_time_es = time.time()\n",
        "\n",
        "# Calculate the errors for Early Stopping\n",
        "X_train_prep_full = preprocessing.transform(X_train)\n",
        "X_test_prep_full = preprocessing.transform(X_test)\n",
        "y_pred_es_train = best_model.predict(X_train_prep_full)\n",
        "y_pred_es_test = best_model.predict(X_test_prep_full)\n",
        "\n",
        "# Calculate performance metrics for each model\n",
        "# Ridge Regression\n",
        "mse_train_ridge = mean_squared_error(y_train, y_pred_ridge_train)\n",
        "mse_test_ridge = mean_squared_error(y_test, y_pred_ridge_test)\n",
        "mae_train_ridge = mean_absolute_error(y_train, y_pred_ridge_train)\n",
        "mae_test_ridge = mean_absolute_error(y_test, y_pred_ridge_test)\n",
        "r2_train_ridge = r2_score(y_train, y_pred_ridge_train)\n",
        "r2_test_ridge = r2_score(y_test, y_pred_ridge_test)\n",
        "time_taken_ridge = end_time_ridge - start_time_ridge\n",
        "\n",
        "# Lasso Regression\n",
        "mse_train_lasso = mean_squared_error(y_train, y_pred_lasso_train)\n",
        "mse_test_lasso = mean_squared_error(y_test, y_pred_lasso_test)\n",
        "mae_train_lasso = mean_absolute_error(y_train, y_pred_lasso_train)\n",
        "mae_test_lasso = mean_absolute_error(y_test, y_pred_lasso_test)\n",
        "r2_train_lasso = r2_score(y_train, y_pred_lasso_train)\n",
        "r2_test_lasso = r2_score(y_test, y_pred_lasso_test)\n",
        "time_taken_lasso = end_time_lasso - start_time_lasso\n",
        "\n",
        "# Elastic Net Regression\n",
        "mse_train_elastic_net = mean_squared_error(y_train, y_pred_elastic_net_train)\n",
        "mse_test_elastic_net = mean_squared_error(y_test, y_pred_elastic_net_test)\n",
        "mae_train_elastic_net = mean_absolute_error(y_train, y_pred_elastic_net_train)\n",
        "mae_test_elastic_net = mean_absolute_error(y_test, y_pred_elastic_net_test)\n",
        "r2_train_elastic_net = r2_score(y_train, y_pred_elastic_net_train)\n",
        "r2_test_elastic_net = r2_score(y_test, y_pred_elastic_net_test)\n",
        "time_taken_elastic_net = end_time_elastic_net - start_time_elastic_net\n",
        "\n",
        "# Early Stopping\n",
        "mse_train_es = mean_squared_error(y_train, y_pred_es_train)\n",
        "mse_test_es = mean_squared_error(y_test, y_pred_es_test)\n",
        "mae_train_es = mean_absolute_error(y_train, y_pred_es_train)\n",
        "mae_test_es = mean_absolute_error(y_test, y_pred_es_test)\n",
        "r2_train_es = r2_score(y_train, y_pred_es_train)\n",
        "r2_test_es = r2_score(y_test, y_pred_es_test)\n",
        "time_taken_es = end_time_es - start_time_es\n",
        "\n",
        "\n",
        "# Store the results in the matrices2 DataFrame (Train & Test metrics)\n",
        "matrices.loc['Ridge Regression'] = [mse_train_ridge, mse_test_ridge, mae_train_ridge, mae_test_ridge, r2_train_ridge, r2_test_ridge, time_taken_ridge]\n",
        "matrices.loc['Lasso Regression'] = [mse_train_lasso, mse_test_lasso, mae_train_lasso, mae_test_lasso, r2_train_lasso, r2_test_lasso, time_taken_lasso]\n",
        "matrices.loc['Elastic Net Regression'] = [mse_train_elastic_net, mse_test_elastic_net, mae_train_elastic_net, mae_test_elastic_net, r2_train_elastic_net, r2_test_elastic_net, time_taken_elastic_net]\n",
        "matrices.loc['Early Stopping'] = [mse_train_es, mse_test_es, mae_train_es, mae_test_es, r2_train_es, r2_test_es, time_taken_es]\n",
        "\n",
        "matrices"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XFI8-wSx9mN2",
      "metadata": {
        "id": "XFI8-wSx9mN2"
      },
      "source": [
        "\n",
        "## 7. Normal Equation\n",
        "**Instruction:** Implement the normal equation method for linear regression.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "PZ0_6G5vZSvP",
      "metadata": {
        "id": "PZ0_6G5vZSvP"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train MSE</th>\n",
              "      <th>Test MSE</th>\n",
              "      <th>Train MAE</th>\n",
              "      <th>Test MAE</th>\n",
              "      <th>Train R2</th>\n",
              "      <th>Test R2</th>\n",
              "      <th>Time Taken</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Simple Linear Regression</th>\n",
              "      <td>5.179331e-01</td>\n",
              "      <td>5.558916e-01</td>\n",
              "      <td>0.528628</td>\n",
              "      <td>0.533200</td>\n",
              "      <td>6.125512e-01</td>\n",
              "      <td>5.757877e-01</td>\n",
              "      <td>0.005100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Polynomial Linear Regression (degree - 2)</th>\n",
              "      <td>4.207266e-01</td>\n",
              "      <td>4.643015e-01</td>\n",
              "      <td>0.460838</td>\n",
              "      <td>0.467001</td>\n",
              "      <td>6.852682e-01</td>\n",
              "      <td>6.456820e-01</td>\n",
              "      <td>0.036937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Polynomial Linear Regression (degree - 3)</th>\n",
              "      <td>3.426077e-01</td>\n",
              "      <td>2.000974e+01</td>\n",
              "      <td>0.416137</td>\n",
              "      <td>0.525636</td>\n",
              "      <td>7.437064e-01</td>\n",
              "      <td>-1.426985e+01</td>\n",
              "      <td>0.199386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Polynomial Linear Regression (degree - 4)</th>\n",
              "      <td>4.706597e-01</td>\n",
              "      <td>2.396746e+03</td>\n",
              "      <td>0.471706</td>\n",
              "      <td>1.605144</td>\n",
              "      <td>6.479149e-01</td>\n",
              "      <td>-1.828006e+03</td>\n",
              "      <td>0.517941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ridge Regression</th>\n",
              "      <td>5.179332e-01</td>\n",
              "      <td>5.558549e-01</td>\n",
              "      <td>0.528624</td>\n",
              "      <td>0.533193</td>\n",
              "      <td>6.125511e-01</td>\n",
              "      <td>5.758157e-01</td>\n",
              "      <td>0.004001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lasso Regression</th>\n",
              "      <td>1.336778e+00</td>\n",
              "      <td>1.310696e+00</td>\n",
              "      <td>0.913911</td>\n",
              "      <td>0.906069</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-2.190871e-04</td>\n",
              "      <td>0.003029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Elastic Net Regression</th>\n",
              "      <td>1.058553e+00</td>\n",
              "      <td>1.044231e+00</td>\n",
              "      <td>0.812107</td>\n",
              "      <td>0.805995</td>\n",
              "      <td>2.081313e-01</td>\n",
              "      <td>2.031260e-01</td>\n",
              "      <td>0.002012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Early Stopping</th>\n",
              "      <td>2.811376e+09</td>\n",
              "      <td>6.395922e+09</td>\n",
              "      <td>2883.486634</td>\n",
              "      <td>3190.668525</td>\n",
              "      <td>-2.103098e+09</td>\n",
              "      <td>-4.880859e+09</td>\n",
              "      <td>3.310140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Normal Equation</th>\n",
              "      <td>5.179331e-01</td>\n",
              "      <td>5.558916e-01</td>\n",
              "      <td>0.528628</td>\n",
              "      <td>0.533200</td>\n",
              "      <td>6.125512e-01</td>\n",
              "      <td>5.757877e-01</td>\n",
              "      <td>0.003016</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Train MSE      Test MSE  \\\n",
              "Simple Linear Regression                   5.179331e-01  5.558916e-01   \n",
              "Polynomial Linear Regression (degree - 2)  4.207266e-01  4.643015e-01   \n",
              "Polynomial Linear Regression (degree - 3)  3.426077e-01  2.000974e+01   \n",
              "Polynomial Linear Regression (degree - 4)  4.706597e-01  2.396746e+03   \n",
              "Ridge Regression                           5.179332e-01  5.558549e-01   \n",
              "Lasso Regression                           1.336778e+00  1.310696e+00   \n",
              "Elastic Net Regression                     1.058553e+00  1.044231e+00   \n",
              "Early Stopping                             2.811376e+09  6.395922e+09   \n",
              "Normal Equation                            5.179331e-01  5.558916e-01   \n",
              "\n",
              "                                             Train MAE     Test MAE  \\\n",
              "Simple Linear Regression                      0.528628     0.533200   \n",
              "Polynomial Linear Regression (degree - 2)     0.460838     0.467001   \n",
              "Polynomial Linear Regression (degree - 3)     0.416137     0.525636   \n",
              "Polynomial Linear Regression (degree - 4)     0.471706     1.605144   \n",
              "Ridge Regression                              0.528624     0.533193   \n",
              "Lasso Regression                              0.913911     0.906069   \n",
              "Elastic Net Regression                        0.812107     0.805995   \n",
              "Early Stopping                             2883.486634  3190.668525   \n",
              "Normal Equation                               0.528628     0.533200   \n",
              "\n",
              "                                               Train R2       Test R2  \\\n",
              "Simple Linear Regression                   6.125512e-01  5.757877e-01   \n",
              "Polynomial Linear Regression (degree - 2)  6.852682e-01  6.456820e-01   \n",
              "Polynomial Linear Regression (degree - 3)  7.437064e-01 -1.426985e+01   \n",
              "Polynomial Linear Regression (degree - 4)  6.479149e-01 -1.828006e+03   \n",
              "Ridge Regression                           6.125511e-01  5.758157e-01   \n",
              "Lasso Regression                           0.000000e+00 -2.190871e-04   \n",
              "Elastic Net Regression                     2.081313e-01  2.031260e-01   \n",
              "Early Stopping                            -2.103098e+09 -4.880859e+09   \n",
              "Normal Equation                            6.125512e-01  5.757877e-01   \n",
              "\n",
              "                                           Time Taken  \n",
              "Simple Linear Regression                     0.005100  \n",
              "Polynomial Linear Regression (degree - 2)    0.036937  \n",
              "Polynomial Linear Regression (degree - 3)    0.199386  \n",
              "Polynomial Linear Regression (degree - 4)    0.517941  \n",
              "Ridge Regression                             0.004001  \n",
              "Lasso Regression                             0.003029  \n",
              "Elastic Net Regression                       0.002012  \n",
              "Early Stopping                               3.310140  \n",
              "Normal Equation                              0.003016  "
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Add the bias term (column of ones) to X_train and X_test\n",
        "X_train_ne = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n",
        "X_test_ne = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
        "\n",
        "# Define the normal equation function\n",
        "def normal_equation(X, y):\n",
        "    return np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
        "\n",
        "# Use the normal equation to find theta\n",
        "start_time_ne = time.time()\n",
        "theta_ne = normal_equation(X_train_ne, y_train)\n",
        "end_time_ne = time.time()\n",
        "\n",
        "# Predict on both training and testing sets\n",
        "y_train_pred_ne = X_train_ne.dot(theta_ne)\n",
        "y_test_pred_ne = X_test_ne.dot(theta_ne)\n",
        "\n",
        "# Calculate training and testing performance metrics\n",
        "mse_train_ne = mean_squared_error(y_train, y_train_pred_ne)\n",
        "mae_train_ne = mean_absolute_error(y_train, y_train_pred_ne)\n",
        "r2_train_ne = r2_score(y_train, y_train_pred_ne)\n",
        "\n",
        "mse_test_ne = mean_squared_error(y_test, y_test_pred_ne)\n",
        "mae_test_ne = mean_absolute_error(y_test, y_test_pred_ne)\n",
        "r2_test_ne = r2_score(y_test, y_test_pred_ne)\n",
        "\n",
        "time_taken_ne = end_time_ne - start_time_ne\n",
        "\n",
        "# Store the results in a DataFrame\n",
        "matrices.loc['Normal Equation'] =[mse_train_ne,mse_test_ne,mae_train_ne,mae_test_ne,r2_train_ne,r2_test_ne,time_taken_ne]\n",
        "\n",
        "matrices"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mfy-eBEe9mN2",
      "metadata": {
        "id": "mfy-eBEe9mN2"
      },
      "source": [
        "## 8. Implement linear Regression using SVD\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "WWx4LfxT9mN3",
      "metadata": {
        "id": "WWx4LfxT9mN3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train MSE</th>\n",
              "      <th>Test MSE</th>\n",
              "      <th>Train MAE</th>\n",
              "      <th>Test MAE</th>\n",
              "      <th>Train R2</th>\n",
              "      <th>Test R2</th>\n",
              "      <th>Time Taken</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Simple Linear Regression</th>\n",
              "      <td>5.179331e-01</td>\n",
              "      <td>5.558916e-01</td>\n",
              "      <td>0.528628</td>\n",
              "      <td>0.533200</td>\n",
              "      <td>6.125512e-01</td>\n",
              "      <td>5.757877e-01</td>\n",
              "      <td>0.005100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Polynomial Linear Regression (degree - 2)</th>\n",
              "      <td>4.207266e-01</td>\n",
              "      <td>4.643015e-01</td>\n",
              "      <td>0.460838</td>\n",
              "      <td>0.467001</td>\n",
              "      <td>6.852682e-01</td>\n",
              "      <td>6.456820e-01</td>\n",
              "      <td>0.036937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Polynomial Linear Regression (degree - 3)</th>\n",
              "      <td>3.426077e-01</td>\n",
              "      <td>2.000974e+01</td>\n",
              "      <td>0.416137</td>\n",
              "      <td>0.525636</td>\n",
              "      <td>7.437064e-01</td>\n",
              "      <td>-1.426985e+01</td>\n",
              "      <td>0.199386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Polynomial Linear Regression (degree - 4)</th>\n",
              "      <td>4.706597e-01</td>\n",
              "      <td>2.396746e+03</td>\n",
              "      <td>0.471706</td>\n",
              "      <td>1.605144</td>\n",
              "      <td>6.479149e-01</td>\n",
              "      <td>-1.828006e+03</td>\n",
              "      <td>0.517941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ridge Regression</th>\n",
              "      <td>5.179332e-01</td>\n",
              "      <td>5.558549e-01</td>\n",
              "      <td>0.528624</td>\n",
              "      <td>0.533193</td>\n",
              "      <td>6.125511e-01</td>\n",
              "      <td>5.758157e-01</td>\n",
              "      <td>0.004001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lasso Regression</th>\n",
              "      <td>1.336778e+00</td>\n",
              "      <td>1.310696e+00</td>\n",
              "      <td>0.913911</td>\n",
              "      <td>0.906069</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-2.190871e-04</td>\n",
              "      <td>0.003029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Elastic Net Regression</th>\n",
              "      <td>1.058553e+00</td>\n",
              "      <td>1.044231e+00</td>\n",
              "      <td>0.812107</td>\n",
              "      <td>0.805995</td>\n",
              "      <td>2.081313e-01</td>\n",
              "      <td>2.031260e-01</td>\n",
              "      <td>0.002012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Early Stopping</th>\n",
              "      <td>2.811376e+09</td>\n",
              "      <td>6.395922e+09</td>\n",
              "      <td>2883.486634</td>\n",
              "      <td>3190.668525</td>\n",
              "      <td>-2.103098e+09</td>\n",
              "      <td>-4.880859e+09</td>\n",
              "      <td>3.310140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Normal Equation</th>\n",
              "      <td>5.179331e-01</td>\n",
              "      <td>5.558916e-01</td>\n",
              "      <td>0.528628</td>\n",
              "      <td>0.533200</td>\n",
              "      <td>6.125512e-01</td>\n",
              "      <td>5.757877e-01</td>\n",
              "      <td>0.003016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SVD Equation</th>\n",
              "      <td>5.179331e-01</td>\n",
              "      <td>5.558916e-01</td>\n",
              "      <td>0.528628</td>\n",
              "      <td>0.533200</td>\n",
              "      <td>6.125512e-01</td>\n",
              "      <td>5.757877e-01</td>\n",
              "      <td>0.010295</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Train MSE      Test MSE  \\\n",
              "Simple Linear Regression                   5.179331e-01  5.558916e-01   \n",
              "Polynomial Linear Regression (degree - 2)  4.207266e-01  4.643015e-01   \n",
              "Polynomial Linear Regression (degree - 3)  3.426077e-01  2.000974e+01   \n",
              "Polynomial Linear Regression (degree - 4)  4.706597e-01  2.396746e+03   \n",
              "Ridge Regression                           5.179332e-01  5.558549e-01   \n",
              "Lasso Regression                           1.336778e+00  1.310696e+00   \n",
              "Elastic Net Regression                     1.058553e+00  1.044231e+00   \n",
              "Early Stopping                             2.811376e+09  6.395922e+09   \n",
              "Normal Equation                            5.179331e-01  5.558916e-01   \n",
              "SVD Equation                               5.179331e-01  5.558916e-01   \n",
              "\n",
              "                                             Train MAE     Test MAE  \\\n",
              "Simple Linear Regression                      0.528628     0.533200   \n",
              "Polynomial Linear Regression (degree - 2)     0.460838     0.467001   \n",
              "Polynomial Linear Regression (degree - 3)     0.416137     0.525636   \n",
              "Polynomial Linear Regression (degree - 4)     0.471706     1.605144   \n",
              "Ridge Regression                              0.528624     0.533193   \n",
              "Lasso Regression                              0.913911     0.906069   \n",
              "Elastic Net Regression                        0.812107     0.805995   \n",
              "Early Stopping                             2883.486634  3190.668525   \n",
              "Normal Equation                               0.528628     0.533200   \n",
              "SVD Equation                                  0.528628     0.533200   \n",
              "\n",
              "                                               Train R2       Test R2  \\\n",
              "Simple Linear Regression                   6.125512e-01  5.757877e-01   \n",
              "Polynomial Linear Regression (degree - 2)  6.852682e-01  6.456820e-01   \n",
              "Polynomial Linear Regression (degree - 3)  7.437064e-01 -1.426985e+01   \n",
              "Polynomial Linear Regression (degree - 4)  6.479149e-01 -1.828006e+03   \n",
              "Ridge Regression                           6.125511e-01  5.758157e-01   \n",
              "Lasso Regression                           0.000000e+00 -2.190871e-04   \n",
              "Elastic Net Regression                     2.081313e-01  2.031260e-01   \n",
              "Early Stopping                            -2.103098e+09 -4.880859e+09   \n",
              "Normal Equation                            6.125512e-01  5.757877e-01   \n",
              "SVD Equation                               6.125512e-01  5.757877e-01   \n",
              "\n",
              "                                           Time Taken  \n",
              "Simple Linear Regression                     0.005100  \n",
              "Polynomial Linear Regression (degree - 2)    0.036937  \n",
              "Polynomial Linear Regression (degree - 3)    0.199386  \n",
              "Polynomial Linear Regression (degree - 4)    0.517941  \n",
              "Ridge Regression                             0.004001  \n",
              "Lasso Regression                             0.003029  \n",
              "Elastic Net Regression                       0.002012  \n",
              "Early Stopping                               3.310140  \n",
              "Normal Equation                              0.003016  \n",
              "SVD Equation                                 0.010295  "
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define SVD equation. Inputs: X and y, Output: theta\n",
        "def svd_equation(X, y):\n",
        "    U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
        "    S_inv = np.diag(1 / s)\n",
        "    theta = Vt.T.dot(S_inv).dot(U.T).dot(y)\n",
        "    return theta\n",
        "\n",
        "# Converting the X_train and X_test to include the bias term\n",
        "X_train_svd = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n",
        "X_test_svd = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
        "\n",
        "# Use the svd equation to find theta\n",
        "start_time_svd = time.time()\n",
        "theta_svd = svd_equation(X_train_svd, y_train)\n",
        "end_time_svd = time.time()\n",
        "\n",
        "# Predict using the svd equation\n",
        "y_train_pred_svd = X_train_svd.dot(theta_svd)\n",
        "y_test_pred_svd = X_test_svd.dot(theta_svd)\n",
        "\n",
        "# Calculate the performance metrics for training and testing\n",
        "mse_train_svd = mean_squared_error(y_train, y_train_pred_svd)\n",
        "mse_test_svd = mean_squared_error(y_test, y_test_pred_svd)\n",
        "mae_train_svd = mean_absolute_error(y_train, y_train_pred_svd)\n",
        "mae_test_svd = mean_absolute_error(y_test, y_test_pred_svd)\n",
        "r2_train_svd = r2_score(y_train, y_train_pred_svd)\n",
        "r2_test_svd = r2_score(y_test, y_test_pred_svd)\n",
        "time_taken_svd = end_time_svd - start_time_svd\n",
        "\n",
        "# Print the performance metrics\n",
        "matrices.loc['SVD Equation'] =[mse_train_svd,mse_test_svd,mae_train_svd,mae_test_svd,r2_train_svd,r2_test_svd,time_taken_svd]\n",
        "\n",
        "matrices"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7Ftk5xD9mN3",
      "metadata": {
        "id": "d7Ftk5xD9mN3"
      },
      "source": [
        "\n",
        "## 9. Performance Metrics and Computational Analysis\n",
        "**Instruction:** Compare the performance and computational time of all models implemented.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "eZjOz8B09mN3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eZjOz8B09mN3",
        "outputId": "bbdc3479-a6bb-4020-ed0d-b4ebc404ac4b"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'MSE'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mKeyError\u001b[0m: 'MSE'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[50], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m width \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Plot MSE, MAE, and R Score comparison directly\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m ax[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbar(x \u001b[38;5;241m-\u001b[39m width, \u001b[43mmatrices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMSE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, width, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n\u001b[0;32m     10\u001b[0m ax[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbar(x, matrices[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE\u001b[39m\u001b[38;5;124m'\u001b[39m], width, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mg\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n\u001b[0;32m     11\u001b[0m ax[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbar(x \u001b[38;5;241m+\u001b[39m width, matrices[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR2\u001b[39m\u001b[38;5;124m'\u001b[39m], width, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR Score\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[1;31mKeyError\u001b[0m: 'MSE'"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x1000 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create the figure and subplots\n",
        "fig, ax = plt.subplots(2, 1, figsize=(8, 10))\n",
        "\n",
        "# Get the number of methods and their names (index)\n",
        "x = np.arange(len(matrices))\n",
        "width = 0.25\n",
        "\n",
        "# Plot MSE, MAE, and R Score comparison directly\n",
        "ax[0].bar(x - width, matrices['MSE'], width, label='MSE', color='b', alpha=0.7)\n",
        "ax[0].bar(x, matrices['MAE'], width, label='MAE', color='g', alpha=0.7)\n",
        "ax[0].bar(x + width, matrices['R2'], width, label='R Score', color='r', alpha=0.7)\n",
        "\n",
        "# Set labels, titles, and ticks for the first plot\n",
        "ax[0].set_xlabel('Regression Methods')\n",
        "ax[0].set_ylabel('MSE / MAE / R Score')\n",
        "ax[0].set_title('Evaluation of Regression Techniques: MSE, MAE, and R Score')\n",
        "ax[0].set_xticks(x)\n",
        "ax[0].set_xticklabels(matrices.index, rotation=45, ha='right')\n",
        "ax[0].legend(loc='upper left')\n",
        "\n",
        "# Plot computational time comparison with log scale\n",
        "ax[1].bar(x, matrices['Time Taken'], width, label='Time Taken (s)', color='y', alpha=0.7)\n",
        "\n",
        "# Set y-axis to logarithmic scale for time\n",
        "ax[1].set_yscale('log')\n",
        "\n",
        "# Set labels, titles, and ticks for the second plot\n",
        "ax[1].set_xlabel('Regression Methods')\n",
        "ax[1].set_ylabel('Time Taken (s)')\n",
        "ax[1].set_title('Time Taken for Regression Methods (Log Scale)')\n",
        "ax[1].set_xticks(x)\n",
        "ax[1].set_xticklabels(matrices.index, rotation=45, ha='right')\n",
        "ax[1].legend(loc='upper left')\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37457f4f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "37457f4f",
        "outputId": "43bb5ace-236f-4a94-d8df-4b8df578ce37"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(3, 1, figsize=(5, 10))\n",
        "\n",
        "# Batch Gradient Descent\n",
        "axes[0].plot(cost_history_bgd_test, color='r')\n",
        "axes[0].set_title(\"Batch Gradient Descent\")\n",
        "axes[0].set_xlabel(\"Epoch\")\n",
        "axes[0].set_ylabel(\"Cost\")\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Stochastic Gradient Descent\n",
        "axes[1].plot(cost_history_sgd_test, color='g')\n",
        "axes[1].set_title(\"Stochastic Gradient Descent\")\n",
        "axes[1].set_xlabel(\"Epoch\")\n",
        "axes[1].set_ylabel(\"Cost\")\n",
        "axes[1].grid(True)\n",
        "\n",
        "# Mini-Batch Gradient Descent\n",
        "axes[2].plot(cost_history_mini_batch_test, color='b')\n",
        "axes[2].set_title(\"Mini-Batch Gradient Descent\")\n",
        "axes[2].set_xlabel(\"Epoch\")\n",
        "axes[2].set_ylabel(\"Cost\")\n",
        "axes[2].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26873213",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "26873213",
        "outputId": "5a61f3c4-79bf-4ed7-f18e-b97ac3759381"
      },
      "outputs": [],
      "source": [
        "# plotting different type of gradient descent\n",
        "# Function to plot training and test cost\n",
        "def plot_costs(cost_history_train, cost_history_test, title):\n",
        "    plt.plot(cost_history_train, label=\"Train Cost\")\n",
        "    plt.plot(cost_history_test, label=\"Test Cost\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Cost')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Plot Batch Gradient Descent costs\n",
        "plot_costs(cost_history_bgd_train, cost_history_bgd_test, \"Batch Gradient Descent\")\n",
        "# Plot Stochastic Gradient Descent costs\n",
        "plot_costs(cost_history_sgd_train, cost_history_sgd_test, \"Stochastic Gradient Descent\")\n",
        "\n",
        "# Plot Mini-Batch Gradient Descent costs\n",
        "plot_costs(cost_history_mini_batch_train, cost_history_mini_batch_test, \"Mini-Batch Gradient Descent\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZoHtLnO09mN3",
      "metadata": {
        "id": "ZoHtLnO09mN3"
      },
      "source": [
        "\n",
        "## 10. Conclusion\n",
        "**Instruction:** Summarize the findings from the analysis, including which models performed best in terms of accuracy and computational efficiency.\n",
        "\n",
        "\n",
        "\n",
        "### Accuracy:\n",
        "- R^2 scores range from about 0.41 to 0.48 for most models\n",
        "- **Best performer**: Polynomial Linear Regression (degree 4)\n",
        "  - Highest R^2 score: 0.4853\n",
        "  - Lowest MSE: 6.86e+09\n",
        "- **Poorest performer**: Elastic Net Regression\n",
        "  - Lowest R^2 score: 0.4173\n",
        "  - Highest MSE: 7.97e+09\n",
        "- Similar performance: Simple Linear Regression, Ridge Regression, Lasso Regression, Early Stopping, Normal Equation, and SVD Equation\n",
        "\n",
        "### Computational Efficiency:\n",
        "- Most methods: 0.001 to 0.004 seconds\n",
        "- **Fastest**: Normal Equation (0 seconds)\n",
        "- **Slowest**: Early Stopping (4.4 seconds)\n",
        "- **Very fast**: SVD Equation and Ridge Regression\n",
        "\n",
        "### Best Overall Performance:\n",
        "- **Polynomial Linear Regression (degree 3 or 4)** - but has slightly longer computation time compared to SVD and Ridge.\n",
        "\n",
        "### Worst Overall Performance:\n",
        "- **Early Stopping** - has very high computation time with no significant accuracy gain."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
